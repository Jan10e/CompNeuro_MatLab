% MATLAB code for Week 6 class in INP599
% March 1, 2017
% Alex Kwan
%
% Week 6 (3/1): Regression / linear regression (Kwan) 

% Prelude. t-tests, paired and unpaired
% Prelude. Non-parametric tests
% 1. Two-factor ANOVA, main and interaction effects
% 2. Correlation
% 3. Simple linear regression
% 4. Multiple linear regression

clear all;
close all;

%% --- Prelude. t-tests
% To test the efficacy of a drug on mice on locomotor activity
% 2 experimental groups, each with 10 male animals
% drug, control

% locomotor activity measurements of the 2 groups
% construct data that will show a clear effect of drug
n=10;   %10 subjects
mCtrl=randn(n,1); %randn, mean of 0 and variance of 1
mDrug=mCtrl+1+randn(n,1);

% if these are different animals, i.e., 10+10=20 total
% 'across-subject'; unpaired data, two-sample t-test
[~,p]=ttest2(mCtrl,mDrug)

% if these are the same animals, e.g. drug then saline, 10 total
% 'within-subject', paired differences, paired t-test

% Caution: Student's t-test assumes normal distribution. Use caution if
% sample sizes or variances are unequal

% History: Why Student? Gosset introduced the test in 1908, while working
% for the Guinness brewery. Developed the test as a way to monitor the
% chemical quality of stout. Company policy at Guiness did not allow 
% publication, so Gosset published the work under the pseudonym 'Student'
[~,p]=ttest(mCtrl,mDrug)

% Expt design: Latin square / randomized block design to counter-balance the 
% experimental conditions. What if the order of testing has an effect (i.e.
% locomotor activity is always higher on the first day of testing?)

%% --- Prelude. non-parametric tests
% But wait, for many data we do not know the underlying distribution.
% For example, calcium imaging data involving fractional change in
% fluorescence (dF/F) where most values are zero, with some large positive
% values.
%
% Non-parametric tests with no assumption about the underlying distribution

% Wilcoxon rank-sum test, also known as Mann-Whitney U test; unpaired data
[p,~]=ranksum(mCtrl,mDrug)

% Wilcoxon signed-rank test; paired data
[p,~]=signrank(mCtrl,mDrug)

%% --- 1. Two-factor ANOVA
% ANOVA - analysis of variance
% Rationale: In complex biological systems, multiple experimental factors 
% may interact to produce effects. For example, multiple drugs can be 
% administered simultaneously, and all pairwise level combinations can be 
% observed in a single experiment, which is more revealing than doing one
% drug at a time. In particular, we would miss insights about synergies.
%
% Why not t-tests? With more than 2 groups, t-tests would be inappropriate 
% because we would need to do many t-test. Effectively from the same 
% popluation, we are  drawing >2 samples and testing differences in mean, 
% thus increasing the chance of a type I error.
%
% Example 1: main effect
% #SABV for #NIHGrant
% To test the efficacy of a drug on mice on locomotor activity
% 4 experimental groups, each with 10 animals
% male/drug, female/drug, male/control, female/control

% locomotor activity measurements of the 4 groups
% construct data that will show a clear effect of drug; there is also an
% effect of sex
% 'complete factorial': every combination is observed
mCtrl=randn(n,1); 
fCtrl=1+randn(n,1);
mDrug=5+randn(n,1);
fDrug=6+randn(n,1);

% -> write a function to plot the data
plot_effect(mCtrl, fCtrl, mDrug, fDrug);

% -> write a function to do two-factor ANOVA
% there is no interaction and the effects are 'additive'
test_effect(mCtrl, fCtrl, mDrug, fDrug);

% how to read the N-Way ANOVA table generated by Matlab
% d.f.: degree of freedom; basically group size minus 1
% F: the F test statistics, often included in papers, with subscripts to
% indicate (#group - 1) and (#sample - #group).
% Prob>F: the p-value
%
% why analysis of 'variance'? The intuition is that this is a comparison of
% the different sources of variances. Here, across-group vs. within-group. 
% If variability is significantly larger across-group, then ANOVA will 
% reject the null hypothesis of equality among population means

%% Example 2: main effect and interaction

% another set of locomotor activity measurements of the 4 groups
% construct data with effect of drug that also differ by sex
mCtrl=randn(n,1);
fCtrl=1+randn(n,1);
mDrug=5+randn(n,1);
fDrug=12+randn(n,1);

plot_effect(mCtrl, fCtrl, mDrug, fDrug);

test_effect(mCtrl, fCtrl, mDrug, fDrug);

%typically if interaction is present, then the test of significance for the
%main effect is ignored.

%% Example 3: interaction effect

% a third set of locomotor activity measurements of the 4 groups
mCtrl=randn(n,1);
fCtrl=3+randn(n,1);
mDrug=3+randn(n,1);
fDrug=randn(n,1);

plot_effect(mCtrl, fCtrl, mDrug, fDrug);
% interaction effects may mask main effects

test_effect(mCtrl, fCtrl, mDrug, fDrug);

% 'Repeated measure': Sometimes for ANOVA there is a longitudinal grouping, 
% e.g., days of treatment. The same subject is treated over multiple days 
% and each day is associated with a data point. This design requires a 
% repeated measures ANOVA.
%
% 'Fixed' vs 'Random effects': Here sex is a fixed effect. All the groups
% are observed. For example, site of experiment would be a random effect, a
% subset of all possible groupings.

%% Example 4: post-hoc multiple comparison
% ANOVA lets us know whether there is a main/interaction effect. Beyond 
% that, it is often useful to go back to look at two specific groups and 
% see if their difference is significant.
%
% To test different mouse strains on locomotor activity
% 1 wild-type and 3 transgenic groups, each with 10 animals
ctrl=randn(n,1);
tg1=0.2+randn(n,1);
tg2=4+randn(n,1);
tg3=6+randn(n,1);

% run a one-way ANOVA
data=[ctrl tg1 tg2 tg3];
strain=[zeros(size(ctrl)) ones(size(tg1)) 2*ones(size(tg2)) 3*ones(size(tg2))];
[~,~,stats,~]=anovan(data(:),strain(:),'model','linear','varnames',{'strain'});

% 'post-hoc multiple comparison'
% note: in the plot, blue is group selected to be compared with all other groups
comp=multcompare(stats);

% look in the variable 'comp' to see the actual p-values
comp

% --> using g*power to estimate sample size

%% --- 2. Correlation and simple linear regression
% To test the efficacy of a drug on mice on locomotor activity
% However, consider now drug is now measured in blood sample 
% instead of categories (drug vs no drug given to animal), 
% now we have numerical values (how much drug)

n=40;
drug=5+randn(n,1);
slope=0.4;
activity=slope*drug+randn(n,1); %linearly related plus some noise

figure;
plot(drug,activity,'ko','MarkerSize',10);
xlabel('Drug dose (mg)'); %independent variable
ylabel('Locomotor activity (m)'); %dependent variable

% simple linear regression, or linear fit
% if we suspect a trend in our data for how Y varies with X,
% the goal is typicallly to find the a best line through the data. So
% there will be a slope and an intercept, and we want to know whether they
% are significantly different from zero
stats=regstats(activity,drug,'linear');
stats.beta
stats.tstat.pval %is the slope significantly different from zero?

% let's plot the slope in the same figure
hold on;
x=[0:0.01:10];
plot(x,stats.beta(1)+x*stats.beta(2));

%% Pearson's correlation coefficients

% Pearson's, measures linear trends, range from -1 to 1
corr(drug,activity,'type','Pearson')

%the squared pearson correlation coefficient r^2 can be interpreted as the
%proporation of variation in Y explained by X, for example r = 0.9 means
%that 81% of the variance of Y is explained by X

%% Spearman's or rank correlation
drug=5+randn(n,1);
activity=10.^drug;
figure;
plot(drug,activity,'k.','MarkerSize',30);

corr(drug,activity,'type','Pearson')
% Spearman's or rank correlation, measures increasing or decreasing trends
corr(drug,activity,'type','Spearman')

%% Effect of noise and sample size on correlation coefficients
drug=5+randn(n,1);
slope=0.4;

activity=slope*drug+0.1*randn(n,1); %linearly related plus some noise
subplot(2,2,1);
plot(drug,activity,'MarkerSize',30);
corr(drug,activity,'type','Pearson')

activity=slope*drug+0.5*randn(n,1); %more noise
subplot(2,2,2);
plot(drug,activity,'MarkerSize',30);
corr(drug,activity,'type','Pearson')

activity=slope*drug+1*randn(n,1); %a lot noise
subplot(2,2,3);
plot(drug,activity,'MarkerSize',30);
corr(drug,activity,'type','Pearson')

%same undelrying trend, but if there is more noise, the calculated
%correlation coefficient value is lower

%% Effect of sample size on correlation coefficients

n=40;
drug=5+randn(n,1);
activity=slope*drug+1*randn(n,1); %linearly related plus some noise

corr(drug,activity,'type','Pearson')

%Re-run this several times -> with a small sample size, estimation of 
%correlation correlation can be quite variable

%given issues with noise and sample size, it is valuable to also include 
%p-value along with report of correlation coefficient
[rho,pval]=corr(drug,activity,'type','Pearson')

%% Residuals as a regression diagnostics
% how do we know if the linear regressino was appropriate? One common way
% is to plot the residual. Residual is the difference between the measured
% and predicted values.

figure;
predicted=stats.beta(1)+drug*stats.beta(2);
residual=activity-predicted;
plot(drug,residual,'k.','MarkerSize',30);
hold on;
plot(drug,zeros(size(drug)));
ylabel('Residual');
ylim([-3 3]);

% the plot should show zero mean, constant spread, and no local trends

% Least-square errors: this illustrate how linear fits are obtained,
% typically through minizing squared residuals

%% Residuals in an example where linear regression is inappropriate

%if instead, the relationship has an additional nonlinear term
n=40;
activity=slope*drug+randn(n,1) + drug.^2;
stats=regstats(activity,drug,'linear');

figure;
plot(drug,activity,'k.','MarkerSize',30);
hold on;
plot(x,stats.beta(1)+x*stats.beta(2));

figure;
predicted=stats.beta(1)+drug*stats.beta(2);
residual=activity-predicted;
plot(drug,residual,'k.','MarkerSize',30);
hold on;
plot(drug,zeros(size(drug)));
ylabel('Residual');

%% Regression to the mean
% Example of a problematic study design, in which we first measure
% locomotor activity, pick out the low-performing mice, and then treat them
% with drug to see if that improves the performance.

% The issue is known as regression to the mean

% first, pick the low performer
n=40;
activity=10+randn(n,1);
cutoff=prctile(activity,33);
%percentile function to pick the lowest 1/3 performer
n=sum(activity<cutoff);
beforeGroup=activity(activity<cutoff);

figure; hold on;
plot(1,activity,'ko','MarkerSize',10);
plot(2,beforeGroup,'ro','MarkerSize',10);

% then, assume the drug has no effect, and the initial spread in 
% performance is due to variability in our measurement (e.g., poor animal
% tracking for locomotor activity)
afterGroup=randsample(activity,n);
plot(3,afterGroup,'ro','MarkerSize',10);

% before compare to after, within subject, so use paired t-test
[~,p]=ttest(beforeGroup,afterGroup)

% The danger is to arbitrarily cut the initial cohort of animals into low-
% and high-performing groups, even though there is no evidence to do so.
% That is, the distribution is normal, not bimodal.
% 
% This kind of regression to mean could be an explanation for placebo
% effect. People seek treatment when they are very sick. Over time they
% tend to 'regress to mean', regardless of whether they receive treatment
% or placebo. This may contribute to placebo effect.

%% multiple linear regression

%back to example of locomotor activity for 4 groups: drug/control, M/F
n=10;
mCtrl=randn(n,1); 
fCtrl=1+randn(n,1);
mDrug=5+randn(n,1);
fDrug=6+randn(n,1);
data=[mCtrl; fCtrl; mDrug; fDrug];

%fit the multiple linear regression equation
factorDrug=[zeros(n,1); zeros(n,1); ones(n,1); ones(n,1)];
factorSex=[zeros(n,1); ones(n,1); zeros(n,1); ones(n,1)];
factors=[factorDrug factorSex];

%terms in the equation
terms=[[0 0]; [1 0]; [0 1]; [1 1]];
    
stats=regstats(data,factors,terms);

%the fitted coefficients and p-values
stats.beta
stats.tstat.pval

%% Similar p-values obtained by MLR and two-factor ANOVA

% increase the sample size
n=1000;
mCtrl=randn(n,1); 
fCtrl=1+randn(n,1);
mDrug=5+randn(n,1);
fDrug=6+randn(n,1);
data=[mCtrl; fCtrl; mDrug; fDrug];
factorDrug=[zeros(n,1); zeros(n,1); ones(n,1); ones(n,1)];
factorSex=[zeros(n,1); ones(n,1); zeros(n,1); ones(n,1)];
factors=[factorDrug factorSex];

stats=regstats(data,factors,terms);
stats.tstat.pval

test_effect(mCtrl, fCtrl, mDrug, fDrug);
